{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# ðŸ·ï¸ Assignment: Building a POS Tagger from Scratch using Hidden Markov Models\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this assignment you will:\n",
    "1. **Learn** an HMM from a labeled corpus (compute Ï€, A, B matrices)\n",
    "2. **Decode** new sentences using the Viterbi algorithm\n",
    "3. **Evaluate** your tagger against NLTK's built-in HMM tagger\n",
    "\n",
    "## Background\n",
    "\n",
    "A first-order HMM for POS tagging is defined by three components:\n",
    "\n",
    "| Component | Symbol | Definition |\n",
    "|-----------|--------|------------|\n",
    "| **Prior probabilities** | Ï€(t) | Probability that tag `t` starts a sentence |\n",
    "| **Transition matrix** | A[tâ‚ â†’ tâ‚‚] | Probability of tag `tâ‚‚` following tag `tâ‚` |\n",
    "| **Emission matrix** | B[t â†’ w] | Probability of word `w` being emitted by tag `t` |\n",
    "\n",
    "All probabilities are estimated by **Maximum Likelihood Estimation (MLE)**:\n",
    "\n",
    "$$\\pi(t) = \\frac{\\text{count}(t \\text{ starts sentence})}{\\text{total sentences}}$$\n",
    "\n",
    "$$A[t_i \\to t_j] = \\frac{\\text{count}(t_i, t_j)}{\\text{count}(t_i \\text{ as non-final tag})}$$\n",
    "\n",
    "$$B[t \\to w] = \\frac{\\text{count}(w, t)}{\\text{count}(t)}$$\n",
    "\n",
    "---\n",
    "\n",
    "## Structure\n",
    "\n",
    "| Section | Task | Who implements? |\n",
    "|---------|------|-----------------|\n",
    "| 1. Corpus | Hardcoded labeled sentences | âœ… Provided |\n",
    "| 2. Exploration | Inspect the data | âœ… Provided |\n",
    "| 3. HMM Learning | Compute Ï€, A, B | ðŸ”§ **You implement** |\n",
    "| 4. Viterbi Decoder | Find best tag sequence | ðŸ”§ **You implement** |\n",
    "| 5. Evaluation | Token-level accuracy | âœ… Provided |\n",
    "| 6. NLTK Comparison | Train NLTK HMM & compare | âœ… Provided |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 â€” Corpus\n",
    "\n",
    "The corpus below contains the **first 10 sentences** of the Penn Treebank (Wall Street Journal section), with gold-standard POS tags. Each sentence is a list of `(word, tag)` tuples.\n",
    "\n",
    "> **Note:** Tags follow the Penn Treebank tagset. Common ones you'll see: `NNP` (proper noun), `NN` (noun), `VBN` (verb past participle), `IN` (preposition), `DT` (determiner), `JJ` (adjective), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "corpus-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded: 10 sentences, 246 tokens\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  CORPUS â€” First 10 sentences of the Penn Treebank (WSJ)\n",
    "#  Source: Marcus et al. (1993). Each entry: (word, POS_tag)\n",
    "# ============================================================\n",
    "\n",
    "CORPUS = [\n",
    "    # Sentence 1\n",
    "    [\n",
    "        (\"Pierre\", \"NNP\"),\n",
    "        (\"Vinken\", \"NNP\"),\n",
    "        (\",\", \",\"),\n",
    "        (\"61\", \"CD\"),\n",
    "        (\"years\", \"NNS\"),\n",
    "        (\"old\", \"JJ\"),\n",
    "        (\",\", \",\"),\n",
    "        (\"will\", \"MD\"),\n",
    "        (\"join\", \"VB\"),\n",
    "        (\"the\", \"DT\"),\n",
    "        (\"board\", \"NN\"),\n",
    "        (\"as\", \"IN\"),\n",
    "        (\"a\", \"DT\"),\n",
    "        (\"nonexecutive\", \"JJ\"),\n",
    "        (\"director\", \"NN\"),\n",
    "        (\"Nov.\", \"NNP\"),\n",
    "        (\"29\", \"CD\"),\n",
    "        (\".\", \".\"),\n",
    "    ],\n",
    "    # Sentence 2\n",
    "    [\n",
    "        (\"Mr.\", \"NNP\"),\n",
    "        (\"Vinken\", \"NNP\"),\n",
    "        (\"is\", \"VBZ\"),\n",
    "        (\"chairman\", \"NN\"),\n",
    "        (\"of\", \"IN\"),\n",
    "        (\"Elsevier\", \"NNP\"),\n",
    "        (\"N.V.\", \"NNP\"),\n",
    "        (\",\", \",\"),\n",
    "        (\"the\", \"DT\"),\n",
    "        (\"Dutch\", \"NNP\"),\n",
    "        (\"publishing\", \"VBG\"),\n",
    "        (\"group\", \"NN\"),\n",
    "        (\".\", \".\"),\n",
    "    ],\n",
    "    # Sentence 3\n",
    "    [\n",
    "        (\"Rudolph\", \"NNP\"),\n",
    "        (\"Agnew\", \"NNP\"),\n",
    "        (\",\", \",\"),\n",
    "        (\"55\", \"CD\"),\n",
    "        (\"years\", \"NNS\"),\n",
    "        (\"old\", \"JJ\"),\n",
    "        (\"and\", \"CC\"),\n",
    "        (\"former\", \"JJ\"),\n",
    "        (\"chairman\", \"NN\"),\n",
    "        (\"of\", \"IN\"),\n",
    "        (\"Consolidated\", \"NNP\"),\n",
    "        (\"Gold\", \"NNP\"),\n",
    "        (\"Fields\", \"NNP\"),\n",
    "        (\"PLC\", \"NNP\"),\n",
    "        (\",\", \",\"),\n",
    "        (\"was\", \"VBD\"),\n",
    "        (\"named\", \"VBN\"),\n",
    "        (\"*-1\", \"*-1\"),\n",
    "        (\"a\", \"DT\"),\n",
    "        (\"nonexecutive\", \"JJ\"),\n",
    "        (\"director\", \"NN\"),\n",
    "        (\"of\", \"IN\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"a\", \"DT\"),\n",
    "        (\"British\", \"JJ\"),\n",
    "        (\"...\", \":\"),\n",
    "        (\"company\", \"NN\"),\n",
    "        (\".\", \".\"),\n",
    "    ],\n",
    "    # Sentence 4\n",
    "    [\n",
    "        (\"A\", \"DT\"),\n",
    "        (\"form\", \"NN\"),\n",
    "        (\"of\", \"IN\"),\n",
    "        (\"asbestos\", \"NN\"),\n",
    "        (\"once\", \"RB\"),\n",
    "        (\"used\", \"VBN\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"to\", \"TO\"),\n",
    "        (\"make\", \"VB\"),\n",
    "        (\"Kent\", \"NNP\"),\n",
    "        (\"cigarettes\", \"NNS\"),\n",
    "        (\"has\", \"VBZ\"),\n",
    "        (\"resurf\", \"VBN\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"in\", \"IN\"),\n",
    "        (\"a\", \"DT\"),\n",
    "        (\"class-action\", \"JJ\"),\n",
    "        (\"suit\", \"NN\"),\n",
    "        (\"brought\", \"VBN\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"against\", \"IN\"),\n",
    "        (\"the\", \"DT\"),\n",
    "        (\"Manville\", \"NNP\"),\n",
    "        (\"Corp.\", \"NNP\"),\n",
    "        (\".\", \".\"),\n",
    "    ],\n",
    "    # Sentence 5\n",
    "    [\n",
    "        (\"Lorillard\", \"NNP\"),\n",
    "        (\"Inc.\", \"NNP\"),\n",
    "        (\",\", \",\"),\n",
    "        (\"the\", \"DT\"),\n",
    "        (\"manufacturer\", \"NN\"),\n",
    "        (\"of\", \"IN\"),\n",
    "        (\"Kent\", \"NNP\"),\n",
    "        (\"cigarettes\", \"NNS\"),\n",
    "        (\",\", \",\"),\n",
    "        (\"must\", \"MD\"),\n",
    "        (\"pay\", \"VB\"),\n",
    "        (\"$\", \"$\"),\n",
    "        (\"6\", \"CD\"),\n",
    "        (\"million\", \"CD\"),\n",
    "        (\"to\", \"TO\"),\n",
    "        (\"settle\", \"VB\"),\n",
    "        (\"a\", \"DT\"),\n",
    "        (\"class\", \"NN\"),\n",
    "        (\"-\", \"HYPH\"),\n",
    "        (\"action\", \"NN\"),\n",
    "        (\"suit\", \"NN\"),\n",
    "        (\"brought\", \"VBN\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"against\", \"IN\"),\n",
    "        (\"it\", \"PRP\"),\n",
    "        (\"by\", \"IN\"),\n",
    "        (\"*-2\", \"*-2\"),\n",
    "        (\"smokers\", \"NNS\"),\n",
    "        (\"who\", \"WP\"),\n",
    "        (\"use\", \"VBP\"),\n",
    "        (\"Kent\", \"NNP\"),\n",
    "        (\"cigarettes\", \"NNS\"),\n",
    "        (\".\", \".\"),\n",
    "    ],\n",
    "    # Sentence 6\n",
    "    [\n",
    "        (\"Attorneys\", \"NNS\"),\n",
    "        (\"have\", \"VBP\"),\n",
    "        (\"also\", \"RB\"),\n",
    "        (\"been\", \"VBN\"),\n",
    "        (\"told\", \"VBN\"),\n",
    "        (\"*-1\", \"*-1\"),\n",
    "        (\"to\", \"TO\"),\n",
    "        (\"consider\", \"VB\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"consolidating\", \"VBG\"),\n",
    "        (\"the\", \"DT\"),\n",
    "        (\"suits\", \"NNS\"),\n",
    "        (\"filed\", \"VBN\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"in\", \"IN\"),\n",
    "        (\"other\", \"JJ\"),\n",
    "        (\"courts\", \"NNS\"),\n",
    "        (\".\", \".\"),\n",
    "    ],\n",
    "    # Sentence 7\n",
    "    [\n",
    "        (\"That\", \"DT\"),\n",
    "        (\"request\", \"NN\"),\n",
    "        (\",\", \",\"),\n",
    "        (\"sent\", \"VBD\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"to\", \"TO\"),\n",
    "        (\"about\", \"IN\"),\n",
    "        (\"200\", \"CD\"),\n",
    "        (\"law\", \"NN\"),\n",
    "        (\"firms\", \"NNS\"),\n",
    "        (\",\", \",\"),\n",
    "        (\"also\", \"RB\"),\n",
    "        (\"was\", \"VBD\"),\n",
    "        (\"adv\", \"RB\"),\n",
    "        (\"*-1\", \"*-1\"),\n",
    "        (\"to\", \"TO\"),\n",
    "        (\"raise\", \"VB\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"the\", \"DT\"),\n",
    "        (\"issue\", \"NN\"),\n",
    "        (\"of\", \"IN\"),\n",
    "        (\"consolidation\", \"NN\"),\n",
    "        (\"in\", \"IN\"),\n",
    "        (\"Houston\", \"NNP\"),\n",
    "        (\",\", \",\"),\n",
    "        (\"where\", \"WRB\"),\n",
    "        (\"the\", \"DT\"),\n",
    "        (\"Manville\", \"NNP\"),\n",
    "        (\"asbestos\", \"NN\"),\n",
    "        (\"cases\", \"NNS\"),\n",
    "        (\"are\", \"VBP\"),\n",
    "        (\"en\", \"RB\"),\n",
    "        (\"route\", \"NN\"),\n",
    "        (\".\", \".\"),\n",
    "    ],\n",
    "    # Sentence 8\n",
    "    [\n",
    "        (\"Texas-based\", \"JJ\"),\n",
    "        (\"Lorillard\", \"NNP\"),\n",
    "        (\"has\", \"VBZ\"),\n",
    "        (\"been\", \"VBN\"),\n",
    "        (\"under\", \"IN\"),\n",
    "        (\"fire\", \"NN\"),\n",
    "        (\"for\", \"IN\"),\n",
    "        (\"its\", \"PRP$\"),\n",
    "        (\"failure\", \"NN\"),\n",
    "        (\"to\", \"TO\"),\n",
    "        (\"warn\", \"VB\"),\n",
    "        (\"smokers\", \"NNS\"),\n",
    "        (\"of\", \"IN\"),\n",
    "        (\"cancer\", \"NN\"),\n",
    "        (\"risks\", \"NNS\"),\n",
    "        (\"from\", \"IN\"),\n",
    "        (\"asbestos\", \"NN\"),\n",
    "        (\"in\", \"IN\"),\n",
    "        (\"the\", \"DT\"),\n",
    "        (\"fibers\", \"NN\"),\n",
    "        (\"used\", \"VBN\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"in\", \"IN\"),\n",
    "        (\"Kent\", \"NNP\"),\n",
    "        (\"cigarettes\", \"NNS\"),\n",
    "        (\",\", \",\"),\n",
    "        (\"which\", \"WDT\"),\n",
    "        (\"were\", \"VBD\"),\n",
    "        (\"made\", \"VBN\"),\n",
    "        (\"*-1\", \"*-1\"),\n",
    "        (\"from\", \"IN\"),\n",
    "        (\"1952\", \"CD\"),\n",
    "        (\"to\", \"TO\"),\n",
    "        (\"1956\", \"CD\"),\n",
    "        (\".\", \".\"),\n",
    "    ],\n",
    "    # Sentence 9\n",
    "    [\n",
    "        (\"It\", \"PRP\"),\n",
    "        (\"isn't\", \"VBZ\"),\n",
    "        (\"clear\", \"JJ\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"whether\", \"IN\"),\n",
    "        (\"*-2\", \"*-2\"),\n",
    "        (\"asbestos\", \"NN\"),\n",
    "        (\"is\", \"VBZ\"),\n",
    "        (\"now\", \"RB\"),\n",
    "        (\"used\", \"VBN\"),\n",
    "        (\"*-1\", \"*-1\"),\n",
    "        (\"in\", \"IN\"),\n",
    "        (\"Kent\", \"NNP\"),\n",
    "        (\"cigarettes\", \"NNS\"),\n",
    "        (\".\", \".\"),\n",
    "    ],\n",
    "    # Sentence 10\n",
    "    [\n",
    "        (\"A\", \"DT\"),\n",
    "        (\"form\", \"NN\"),\n",
    "        (\"of\", \"IN\"),\n",
    "        (\"asbestos\", \"NN\"),\n",
    "        (\"once\", \"RB\"),\n",
    "        (\"used\", \"VBN\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"to\", \"TO\"),\n",
    "        (\"make\", \"VB\"),\n",
    "        (\"Kent\", \"NNP\"),\n",
    "        (\"cigarettes\", \"NNS\"),\n",
    "        (\"has\", \"VBZ\"),\n",
    "        (\"led\", \"VBN\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"to\", \"TO\"),\n",
    "        (\"a\", \"DT\"),\n",
    "        (\"class\", \"NN\"),\n",
    "        (\"-\", \"HYPH\"),\n",
    "        (\"action\", \"NN\"),\n",
    "        (\"suit\", \"NN\"),\n",
    "        (\"brought\", \"VBN\"),\n",
    "        (\"*\", \"*\"),\n",
    "        (\"against\", \"IN\"),\n",
    "        (\"the\", \"DT\"),\n",
    "        (\"Manville\", \"NNP\"),\n",
    "        (\"Corp.\", \"NNP\"),\n",
    "        (\".\", \".\"),\n",
    "    ],\n",
    "]\n",
    "\n",
    "print(f\"Corpus loaded: {len(CORPUS)} sentences, {sum(len(s) for s in CORPUS)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 â€” Corpus Exploration\n",
    "\n",
    "Before building the model, let's understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "exploration-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size : 128 unique words\n",
      "Tag set size    : 30 unique tags\n",
      "\n",
      "Tags: ['$', '*', '*-1', '*-2', ',', '.', ':', 'CC', 'CD', 'DT', 'HYPH', 'IN', 'JJ', 'MD', 'NN', 'NNP', 'NNS', 'PRP', 'PRP$', 'RB', 'TO', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WRB']\n",
      "\n",
      "Top 10 most frequent tags:\n",
      "  NN      : 31\n",
      "  NNP     : 29\n",
      "  IN      : 25\n",
      "  DT      : 18\n",
      "  NNS     : 16\n",
      "  VBN     : 15\n",
      "  *       : 14\n",
      "  ,       : 11\n",
      "  JJ      : 10\n",
      "  .       : 10\n",
      "\n",
      "First sentence:\n",
      "  Words : ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "  Tags  : ['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# --- Extract vocabulary and tag set ---\n",
    "all_tags = sorted(set(tag for sent in CORPUS for word, tag in sent))\n",
    "all_words = sorted(set(word for sent in CORPUS for word, _ in sent))\n",
    "\n",
    "print(f\"Vocabulary size : {len(all_words)} unique words\")\n",
    "print(f\"Tag set size    : {len(all_tags)} unique tags\")\n",
    "print(f\"\\nTags: {all_tags}\")\n",
    "\n",
    "# --- Tag frequency ---\n",
    "tag_freq = Counter(tag for sent in CORPUS for _, tag in sent)\n",
    "print(\"\\nTop 10 most frequent tags:\")\n",
    "for tag, count in tag_freq.most_common(10):\n",
    "    print(f\"  {tag:8s}: {count}\")\n",
    "\n",
    "# --- Preview first sentence ---\n",
    "print(\"\\nFirst sentence:\")\n",
    "print(\"  Words :\", [w for w, _ in CORPUS[0]])\n",
    "print(\"  Tags  :\", [t for _, t in CORPUS[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 â€” ðŸ”§ HMM Learning  *(Your implementation)*\n",
    "\n",
    "Implement the three functions below to compute the HMM parameters from the corpus.\n",
    "\n",
    "### Smoothing\n",
    "\n",
    "To avoid **zero probabilities** for unseen events (words or transitions not in the training data), apply **Laplace (add-1) smoothing**:\n",
    "\n",
    "$$A_{smooth}[t_i \\to t_j] = \\frac{\\text{count}(t_i, t_j) + 1}{\\text{count}(t_i) + |\\text{Tags}|}$$\n",
    "\n",
    "$$B_{smooth}[t \\to w] = \\frac{\\text{count}(w, t) + 1}{\\text{count}(t) + |\\text{Vocab}|}$$\n",
    "\n",
    "> **Why smoothing matters:** In Viterbi, multiplying by a zero probability kills any path through that state. Smoothing keeps all paths alive with a small non-zero probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "learning-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def compute_prior(corpus, tags):\n",
    "    \"\"\"\n",
    "    Compute initial state probabilities pi.\n",
    "\n",
    "    pi[t] = count(sentences starting with tag t) / total sentences\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list of list of (word, tag) tuples\n",
    "    tags   : sorted list of all unique tags\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi : dict  {tag -> probability}\n",
    "\n",
    "    Hint\n",
    "    ----\n",
    "    - The starting tag of sentence s is: s[0][1]  (index 0 = first token, index 1 = tag)\n",
    "    - No smoothing needed here; unseen start tags simply get probability 0.\n",
    "    \"\"\"\n",
    "    pi = {t: 0.0 for t in tags}\n",
    "\n",
    "    # â”€â”€ YOUR CODE HERE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    counter = {t: 0 for t in tags}\n",
    "    total_sentences = len(corpus)\n",
    "    for sentence in corpus:\n",
    "        found_tag = sentence[0][1]\n",
    "        counter[found_tag] += 1\n",
    "\n",
    "    for t in tags:\n",
    "        pi[t] = counter[t] / total_sentences\n",
    "\n",
    "    return pi\n",
    "\n",
    "\n",
    "def compute_transition(corpus, tags):\n",
    "    \"\"\"\n",
    "    Compute the transition matrix A with Laplace smoothing.\n",
    "\n",
    "    A[t1][t2] = (count(t1 -> t2) + 1) / (count(t1 as non-final) + |tags|)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list of list of (word, tag) tuples\n",
    "    tags   : sorted list of all unique tags\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : dict of dict  {tag -> {tag -> probability}}\n",
    "        A[t1][t2] is the probability of t2 following t1.\n",
    "\n",
    "    Hints\n",
    "    -----\n",
    "    - Iterate over consecutive pairs (sent[i], sent[i+1]) in each sentence.\n",
    "    - A tag at position len(sent)-1 (last position) is NOT a valid source,\n",
    "      so don't count it in the denominator.\n",
    "    - After counting, divide each count by (row_total + len(tags)) for smoothing.\n",
    "    \"\"\"\n",
    "    # Raw bigram counts\n",
    "    trans_count = defaultdict(lambda: defaultdict(int))\n",
    "    src_count = defaultdict(\n",
    "        int\n",
    "    )  # how many times each tag appears as a non-final source\n",
    "\n",
    "    # â”€â”€ YOUR CODE HERE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    for sentence in CORPUS:\n",
    "        for i in range(len(sentence) - 1):\n",
    "            t1 = sentence[i][1]\n",
    "            t2 = sentence[i + 1][1]\n",
    "\n",
    "            trans_count[t1][t2] += 1\n",
    "            src_count[t1] += 1\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    # Convert counts to smoothed probabilities\n",
    "    A = {}\n",
    "    for t1 in tags:\n",
    "        A[t1] = {}\n",
    "        denom = src_count[t1] + len(tags)  # Laplace denominator\n",
    "        for t2 in tags:\n",
    "            A[t1][t2] = (trans_count[t1][t2] + 1) / denom\n",
    "    return A\n",
    "\n",
    "\n",
    "def compute_emission(corpus, tags, vocab):\n",
    "    \"\"\"\n",
    "    Compute the emission matrix B with Laplace smoothing.\n",
    "\n",
    "    B[t][w] = (count(w tagged as t) + 1) / (count(t) + |vocab|)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list of list of (word, tag) tuples\n",
    "    tags   : sorted list of all unique tags\n",
    "    vocab  : sorted list of all unique words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    B : dict of dict  {tag -> {word -> probability}}\n",
    "        B[t][w] is the probability of word w being emitted by tag t.\n",
    "\n",
    "    Hints\n",
    "    -----\n",
    "    - Count every (word, tag) pair in the corpus.\n",
    "    - Also count total occurrences of each tag (tag_count[t]).\n",
    "    - The smoothed probability for an UNSEEN word w' under tag t is: 1 / (tag_count[t] + |vocab|)\n",
    "    \"\"\"\n",
    "    emit_count = defaultdict(lambda: defaultdict(int))\n",
    "    tag_count = defaultdict(int)\n",
    "\n",
    "    # â”€â”€ YOUR CODE HERE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for sentence in CORPUS:\n",
    "        for word, tag in sentence:\n",
    "            emit_count[tag][word] += 1\n",
    "            tag_count[tag] += 1\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    # Convert counts to smoothed probabilities\n",
    "    B = {}\n",
    "    for t in tags:\n",
    "        B[t] = {}\n",
    "        denom = tag_count[t] + len(vocab)  # Laplace denominator\n",
    "        for w in vocab:\n",
    "            B[t][w] = (emit_count[t][w] + 1) / denom\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "train-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sanity Checks ===\n",
      "Sum of pi            : 1.0000  (expected: 1.0)\n",
      "A rows sum to 1      : True   (sample NNP row: 1.0000)\n",
      "B rows sum to 1      : True   (sample NN row: 1.0000)\n",
      "\n",
      "=== Spot-check Expected Values ===\n",
      "pi['NNP']            : 0.4000  (expected: 0.4000)\n",
      "pi['DT']             : 0.3000  (expected: 0.3000)\n",
      "pi['VBZ']            : 0.0000  (expected: 0.0000)\n",
      "A['DT']['NN'] (raw)  : ~0.50  (unsmoothed count: 9/18)\n",
      "A['DT']['NN'] (smooth): 0.2083\n",
      "A['NNP']['NNP'](smth): 0.1864\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Train the HMM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pi = compute_prior(CORPUS, all_tags)\n",
    "A = compute_transition(CORPUS, all_tags)\n",
    "B = compute_emission(CORPUS, all_tags, all_words)\n",
    "\n",
    "# â”€â”€ Sanity checks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=== Sanity Checks ===\")\n",
    "\n",
    "# 1. pi sums to 1\n",
    "print(f\"Sum of pi            : {sum(pi.values()):.4f}  (expected: 1.0)\")\n",
    "\n",
    "# 2. Each row of A sums to 1\n",
    "row_sums = {t: sum(A[t].values()) for t in all_tags}\n",
    "all_ok = all(abs(s - 1.0) < 1e-9 for s in row_sums.values())\n",
    "print(\n",
    "    f\"A rows sum to 1      : {all_ok}   (sample NNP row: {sum(A['NNP'].values()):.4f})\"\n",
    ")\n",
    "\n",
    "# 3. Each row of B sums to 1\n",
    "b_row_sums = {t: sum(B[t].values()) for t in all_tags}\n",
    "all_ok_b = all(abs(s - 1.0) < 1e-9 for s in b_row_sums.values())\n",
    "print(\n",
    "    f\"B rows sum to 1      : {all_ok_b}   (sample NN row: {sum(B['NN'].values()):.4f})\"\n",
    ")\n",
    "\n",
    "# 4. Spot-check specific values\n",
    "print(\"\\n=== Spot-check Expected Values ===\")\n",
    "print(f\"pi['NNP']            : {pi['NNP']:.4f}  (expected: 0.4000)\")\n",
    "print(f\"pi['DT']             : {pi['DT']:.4f}  (expected: 0.3000)\")\n",
    "print(f\"pi['VBZ']            : {pi['VBZ']:.4f}  (expected: 0.0000)\")\n",
    "print(f\"A['DT']['NN'] (raw)  : ~0.50  (unsmoothed count: 9/18)\")\n",
    "print(f\"A['DT']['NN'] (smooth): {A['DT']['NN']:.4f}\")\n",
    "print(f\"A['NNP']['NNP'](smth): {A['NNP']['NNP']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 â€” ðŸ”§ Viterbi Decoder  *(Your implementation)*\n",
    "\n",
    "The Viterbi algorithm finds the **most probable tag sequence** for an observed word sequence.\n",
    "\n",
    "It fills a trellis of size `|states| Ã— |observations|`:\n",
    "\n",
    "$$\\delta(s, 1) = \\pi(s) \\times B[s \\to o_1]$$\n",
    "\n",
    "$$\\delta(s, t) = \\max_{s'} \\left[ \\delta(s', t-1) \\times A[s' \\to s] \\right] \\times B[s \\to o_t]$$\n",
    "\n",
    "$$\\psi(s, t) = \\operatorname{argmax}_{s'} \\left[ \\delta(s', t-1) \\times A[s' \\to s] \\right]$$\n",
    "\n",
    "The best tag sequence is recovered by **backtracking** from the final state with the highest `Î´`.\n",
    "\n",
    "### âš ï¸ Numerical Underflow\n",
    "\n",
    "Multiplying many small probabilities causes **floating-point underflow** (values become 0). Use **log probabilities** instead:\n",
    "\n",
    "$$\\log \\delta(s, t) = \\max_{s'} \\left[ \\log \\delta(s', t-1) + \\log A[s' \\to s] \\right] + \\log B[s \\to o_t]$$\n",
    "\n",
    "> Replace every multiplication with **addition** and every max-product with **max-sum**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "viterbi-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "LOG_ZERO = float(\"-inf\")  # represents log(0)\n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "    \"\"\"Return log(x), or LOG_ZERO if x <= 0.\"\"\"\n",
    "    return math.log(x) if x > 0 else LOG_ZERO\n",
    "\n",
    "\n",
    "def viterbi(words, tags, pi, A, B):\n",
    "    \"\"\"\n",
    "    Viterbi algorithm â€” returns the most likely POS tag sequence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words : list of str   â€” the observed word sequence\n",
    "    tags  : list of str   â€” all possible POS tags\n",
    "    pi    : dict          â€” prior probabilities  {tag -> prob}\n",
    "    A     : dict of dict  â€” transition matrix    {tag -> {tag -> prob}}\n",
    "    B     : dict of dict  â€” emission matrix      {tag -> {word -> prob}}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_path : list of str  â€” predicted tag for each word\n",
    "\n",
    "    Hints\n",
    "    -----\n",
    "    - Use two tables of size len(tags) x len(words):\n",
    "        delta[t][i]  : best log-probability of reaching tag t at position i\n",
    "        psi[t][i]    : index of the best previous tag that led to (t, i)\n",
    "\n",
    "    - Initialization (i = 0):\n",
    "        delta[t][0] = safe_log(pi[t]) + safe_log(B[t].get(words[0], 0))\n",
    "        psi[t][0]   = None\n",
    "\n",
    "    - Recursion (i > 0), for each tag t:\n",
    "        For each previous tag t_prev, compute:\n",
    "            score = delta[t_prev][i-1] + safe_log(A[t_prev][t])\n",
    "        Take the maximum score and multiply by the emission:\n",
    "            delta[t][i] = max_score + safe_log(B[t].get(words[i], 0))\n",
    "            psi[t][i]   = index of t_prev with highest score\n",
    "\n",
    "    - Termination:\n",
    "        best_last_tag = tag with highest delta[t][-1]\n",
    "\n",
    "    - Backtracking:\n",
    "        Start from best_last_tag and follow psi pointers backwards.\n",
    "        Reverse the collected path before returning.\n",
    "\n",
    "    - OOV (out-of-vocabulary) words:\n",
    "        If words[i] is not in B[t], use the smoothed unseen probability.\n",
    "        With Laplace smoothing this is already handled if you use:\n",
    "            B[t].get(words[i], 1 / (tag_count[t] + len(vocab)))\n",
    "        For simplicity here, a small constant like 1e-6 also works.\n",
    "    \"\"\"\n",
    "    n = len(words)\n",
    "    T = len(tags)\n",
    "    tag_index = {t: i for i, t in enumerate(tags)}  # tag -> integer index\n",
    "\n",
    "    # Trellis tables (T x n)\n",
    "    delta = [[LOG_ZERO] * n for _ in range(T)]\n",
    "    psi = [[None] * n for _ in range(T)]\n",
    "\n",
    "    # â”€â”€ Initialization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # â”€â”€ YOUR CODE HERE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for t in tags:\n",
    "        ti = tag_index[t]\n",
    "        delta[ti][0] = safe_log(pi[t]) + safe_log(B[t].get(words[0], 0))\n",
    "        psi[ti][0] = None\n",
    "\n",
    "    # â”€â”€ Recursion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # â”€â”€ YOUR CODE HERE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for i in range(1, n):\n",
    "        for t in tags:\n",
    "            ti = tag_index[t]\n",
    "            best_score = LOG_ZERO\n",
    "            best_prev_idx = None\n",
    "            for t_prev in tags:\n",
    "                tpi = tag_index[t_prev]\n",
    "                score = delta[tpi][i - 1] + safe_log(A[t_prev].get(t, 0))\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_prev_idx = tpi\n",
    "            delta[ti][i] = best_score + safe_log(B[t].get(words[i], 0))\n",
    "            psi[ti][i] = best_prev_idx\n",
    "\n",
    "    # â”€â”€ Termination â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # â”€â”€ YOUR CODE HERE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    best_last_idx = max(range(T), key=lambda ti: delta[ti][n - 1])\n",
    "\n",
    "    # â”€â”€ Backtracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # â”€â”€ YOUR CODE HERE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    path = [best_last_idx]\n",
    "    for i in range(n - 1, 0, -1):\n",
    "        path.append(psi[path[-1]][i])\n",
    "    path.reverse()\n",
    "    best_path = [tags[ti] for ti in path]\n",
    "\n",
    "    return best_path  # list of tags, same length as words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "decode-test-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 9 decoding:\n",
      "Word                Gold    Predicted    Match\n",
      "------------------------------------------------\n",
      "It                   PRP           DT        âœ—\n",
      "isn't                VBZ           NN        âœ—\n",
      "clear                 JJ          VBN        âœ—\n",
      "*                      *            *        âœ“\n",
      "whether               IN           IN        âœ“\n",
      "*-2                  *-2           DT        âœ—\n",
      "asbestos              NN           NN        âœ“\n",
      "is                   VBZ           IN        âœ—\n",
      "now                   RB           NN        âœ—\n",
      "used                 VBN          VBN        âœ“\n",
      "*-1                  *-1          *-1        âœ“\n",
      "in                    IN           IN        âœ“\n",
      "Kent                 NNP          NNP        âœ“\n",
      "cigarettes           NNS          NNS        âœ“\n",
      ".                      .            .        âœ“\n",
      "\n",
      "Token accuracy on sentence 9: 9/15 = 60.00%\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Quick test on Sentence 9 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "test_sent = CORPUS[8]  # 0-indexed: sentence 9\n",
    "test_words = [w for w, _ in test_sent]\n",
    "gold_tags = [t for _, t in test_sent]\n",
    "\n",
    "pred_tags = viterbi(test_words, all_tags, pi, A, B)\n",
    "\n",
    "print(\"Sentence 9 decoding:\")\n",
    "print(f\"{'Word':<15} {'Gold':>8} {'Predicted':>12} {'Match':>8}\")\n",
    "print(\"-\" * 48)\n",
    "for w, g, p in zip(test_words, gold_tags, pred_tags):\n",
    "    match = \"âœ“\" if g == p else \"âœ—\"\n",
    "    print(f\"{w:<15} {g:>8} {p:>12} {match:>8}\")\n",
    "\n",
    "correct = sum(g == p for g, p in zip(gold_tags, pred_tags))\n",
    "print(\n",
    "    f\"\\nToken accuracy on sentence 9: {correct}/{len(gold_tags)} = {correct/len(gold_tags):.2%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 â€” Evaluation\n",
    "\n",
    "Evaluate your tagger on all 10 training sentences. Since we're training and testing on the same data here (no separate test set due to the small corpus size), this measures **training accuracy** â€” how well the HMM memorized the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eval-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "  My HMM Tagger\n",
      "==================================================\n",
      "  Sent   Correct   Total   Accuracy\n",
      "--------------------------------------\n",
      "     1        17      18     94.44%\n",
      "     2        11      13     84.62%\n",
      "     3        24      28     85.71%\n",
      "     4        25      25    100.00%\n",
      "     5        26      33     78.79%\n",
      "     6        13      18     72.22%\n",
      "     7        25      34     73.53%\n",
      "     8        30      35     85.71%\n",
      "     9         9      15     60.00%\n",
      "    10        25      27     92.59%\n",
      "--------------------------------------\n",
      " TOTAL       205     246     83.33%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(corpus, tags, pi, A, B, label=\"My HMM Tagger\"):\n",
    "    \"\"\"\n",
    "    Run Viterbi on every sentence and compute token-level accuracy.\n",
    "    Returns a dict with per-sentence and overall accuracy.\n",
    "    \"\"\"\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    results = []\n",
    "\n",
    "    for i, sent in enumerate(corpus):\n",
    "        words = [w for w, _ in sent]\n",
    "        gold = [t for _, t in sent]\n",
    "        predicted = viterbi(words, tags, pi, A, B)\n",
    "        correct = sum(g == p for g, p in zip(gold, predicted))\n",
    "        results.append(\n",
    "            {\n",
    "                \"sentence\": i + 1,\n",
    "                \"correct\": correct,\n",
    "                \"total\": len(gold),\n",
    "                \"accuracy\": correct / len(gold),\n",
    "            }\n",
    "        )\n",
    "        total_correct += correct\n",
    "        total_tokens += len(gold)\n",
    "\n",
    "    overall = total_correct / total_tokens\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  {label}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"{'Sent':>6} {'Correct':>9} {'Total':>7} {'Accuracy':>10}\")\n",
    "    print(\"-\" * 38)\n",
    "    for r in results:\n",
    "        print(\n",
    "            f\"{r['sentence']:>6} {r['correct']:>9} {r['total']:>7} {r['accuracy']:>10.2%}\"\n",
    "        )\n",
    "    print(\"-\" * 38)\n",
    "    print(f\"{'TOTAL':>6} {total_correct:>9} {total_tokens:>7} {overall:>10.2%}\")\n",
    "\n",
    "    return {\n",
    "        \"per_sentence\": results,\n",
    "        \"overall\": overall,\n",
    "        \"total_correct\": total_correct,\n",
    "        \"total_tokens\": total_tokens,\n",
    "    }\n",
    "\n",
    "\n",
    "my_results = evaluate(CORPUS, all_tags, pi, A, B, label=\"My HMM Tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 â€” NLTK HMM Tagger (Comparison)\n",
    "\n",
    "We now train NLTK's built-in HMM tagger on the **same 10 sentences** and compare its performance to yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "nltk-train-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK HMM tagger trained successfully.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import hmm as nltk_hmm\n",
    "\n",
    "# NLTK expects a list of sentences, each sentence being a list of (word, tag) tuples\n",
    "trainer = nltk_hmm.HiddenMarkovModelTrainer()\n",
    "nltk_tagger = trainer.train_supervised(CORPUS)\n",
    "\n",
    "print(\"NLTK HMM tagger trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "nltk-eval-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "  NLTK HMM Tagger\n",
      "==================================================\n",
      "  Sent   Correct   Total   Accuracy\n",
      "--------------------------------------\n",
      "     1        18      18    100.00%\n",
      "     2        13      13    100.00%\n",
      "     3        28      28    100.00%\n",
      "     4        25      25    100.00%\n",
      "     5        33      33    100.00%\n",
      "     6        18      18    100.00%\n",
      "     7        34      34    100.00%\n",
      "     8        35      35    100.00%\n",
      "     9        15      15    100.00%\n",
      "    10        27      27    100.00%\n",
      "--------------------------------------\n",
      " TOTAL       246     246    100.00%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_nltk(corpus, nltk_tagger, label=\"NLTK HMM Tagger\"):\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    results = []\n",
    "\n",
    "    for i, sent in enumerate(corpus):\n",
    "        words = [w for w, _ in sent]\n",
    "        gold = [t for _, t in sent]\n",
    "        predicted = [tag for _, tag in nltk_tagger.tag(words)]\n",
    "        correct = sum(g == p for g, p in zip(gold, predicted))\n",
    "        results.append(\n",
    "            {\n",
    "                \"sentence\": i + 1,\n",
    "                \"correct\": correct,\n",
    "                \"total\": len(gold),\n",
    "                \"accuracy\": correct / len(gold),\n",
    "            }\n",
    "        )\n",
    "        total_correct += correct\n",
    "        total_tokens += len(gold)\n",
    "\n",
    "    overall = total_correct / total_tokens\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  {label}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"{'Sent':>6} {'Correct':>9} {'Total':>7} {'Accuracy':>10}\")\n",
    "    print(\"-\" * 38)\n",
    "    for r in results:\n",
    "        print(\n",
    "            f\"{r['sentence']:>6} {r['correct']:>9} {r['total']:>7} {r['accuracy']:>10.2%}\"\n",
    "        )\n",
    "    print(\"-\" * 38)\n",
    "    print(f\"{'TOTAL':>6} {total_correct:>9} {total_tokens:>7} {overall:>10.2%}\")\n",
    "\n",
    "    return {\n",
    "        \"per_sentence\": results,\n",
    "        \"overall\": overall,\n",
    "        \"total_correct\": total_correct,\n",
    "        \"total_tokens\": total_tokens,\n",
    "    }\n",
    "\n",
    "\n",
    "nltk_results = evaluate_nltk(CORPUS, nltk_tagger, label=\"NLTK HMM Tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "compare-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "  FINAL COMPARISON\n",
      "=======================================================\n",
      "  Tagger                      Correct   Total   Accuracy\n",
      "-------------------------------------------------------\n",
      "  My HMM Tagger                   205     246     83.33%\n",
      "  NLTK HMM Tagger                 246     246    100.00%\n",
      "=======================================================\n",
      "\n",
      "ðŸ“Š NLTK outperforms your tagger by 16.67%.\n",
      "   Check your smoothing and probability normalization.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Side-by-side comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"  FINAL COMPARISON\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  {'Tagger':<25} {'Correct':>9} {'Total':>7} {'Accuracy':>10}\")\n",
    "print(\"-\" * 55)\n",
    "print(\n",
    "    f\"  {'My HMM Tagger':<25} \"\n",
    "    f\"{my_results['total_correct']:>9} \"\n",
    "    f\"{my_results['total_tokens']:>7} \"\n",
    "    f\"{my_results['overall']:>10.2%}\"\n",
    ")\n",
    "print(\n",
    "    f\"  {'NLTK HMM Tagger':<25} \"\n",
    "    f\"{nltk_results['total_correct']:>9} \"\n",
    "    f\"{nltk_results['total_tokens']:>7} \"\n",
    "    f\"{nltk_results['overall']:>10.2%}\"\n",
    ")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "diff = my_results[\"overall\"] - nltk_results[\"overall\"]\n",
    "if abs(diff) < 0.01:\n",
    "    print(\"\\nâœ… Your tagger matches NLTK's accuracy closely!\")\n",
    "elif diff > 0:\n",
    "    print(f\"\\nðŸŽ‰ Your tagger outperforms NLTK by {diff:.2%}!\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š NLTK outperforms your tagger by {abs(diff):.2%}.\")\n",
    "    print(\"   Check your smoothing and probability normalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-analysis-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7 â€” Error Analysis\n",
    "\n",
    "Let's look at where your tagger makes mistakes and compare with NLTK's decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "error-analysis-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                   Gold    My Pred    NLTK Pred   My OK   NLTK OK\n",
      "----------------------------------------------------------------------\n",
      "Nov.                    NNP         IN          NNP       âœ—         âœ“\n",
      "Dutch                   NNP         NN          NNP       âœ—         âœ“\n",
      "publishing              VBG         IN          VBG       âœ—         âœ“\n",
      "and                      CC         NN           CC       âœ—         âœ“\n",
      "former                   JJ         IN           JJ       âœ—         âœ“\n",
      "British                  JJ         NN           JJ       âœ—         âœ“\n",
      "...                       :         IN            :       âœ—         âœ“\n",
      "-                      HYPH         IN         HYPH       âœ—         âœ“\n",
      "action                   NN         DT           NN       âœ—         âœ“\n",
      "it                      PRP         NN          PRP       âœ—         âœ“\n",
      "*-2                     *-2        NNP          *-2       âœ—         âœ“\n",
      "smokers                 NNS        NNP          NNS       âœ—         âœ“\n",
      "who                      WP        NNP           WP       âœ—         âœ“\n",
      "use                     VBP        NNP          VBP       âœ—         âœ“\n",
      "Attorneys               NNS         DT          NNS       âœ—         âœ“\n",
      "have                    VBP         NN          VBP       âœ—         âœ“\n",
      "consolidating           VBG         IN          VBG       âœ—         âœ“\n",
      "suits                   NNS         NN          NNS       âœ—         âœ“\n",
      "other                    JJ        NNP           JJ       âœ—         âœ“\n",
      "That                     DT        NNP           DT       âœ—         âœ“\n",
      "request                  NN        NNP           NN       âœ—         âœ“\n",
      "about                    IN         VB           IN       âœ—         âœ“\n",
      "200                      CD         DT           CD       âœ—         âœ“\n",
      "adv                      RB        VBN           RB       âœ—         âœ“\n",
      "Manville                NNP         JJ          NNP       âœ—         âœ“\n",
      "cases                   NNS         IN          NNS       âœ—         âœ“\n",
      "are                     VBP         NN          VBP       âœ—         âœ“\n",
      "en                       RB         IN           RB       âœ—         âœ“\n",
      "Texas-based              JJ        NNP           JJ       âœ—         âœ“\n",
      "its                    PRP$         DT         PRP$       âœ—         âœ“\n",
      "smokers                 NNS          *          NNS       âœ—         âœ“\n",
      "cancer                   NN         DT           NN       âœ—         âœ“\n",
      "risks                   NNS         NN          NNS       âœ—         âœ“\n",
      "It                      PRP         DT          PRP       âœ—         âœ“\n",
      "isn't                   VBZ         NN          VBZ       âœ—         âœ“\n",
      "clear                    JJ        VBN           JJ       âœ—         âœ“\n",
      "*-2                     *-2         DT          *-2       âœ—         âœ“\n",
      "is                      VBZ         IN          VBZ       âœ—         âœ“\n",
      "now                      RB         NN           RB       âœ—         âœ“\n",
      "-                      HYPH         IN         HYPH       âœ—         âœ“\n",
      "action                   NN         DT           NN       âœ—         âœ“\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"{'Word':<18} {'Gold':>8} {'My Pred':>10} {'NLTK Pred':>12} {'My OK':>7} {'NLTK OK':>9}\"\n",
    ")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for sent in CORPUS:\n",
    "    words = [w for w, _ in sent]\n",
    "    gold = [t for _, t in sent]\n",
    "    my_pred = viterbi(words, all_tags, pi, A, B)\n",
    "    nltk_pred = [tag for _, tag in nltk_tagger.tag(words)]\n",
    "\n",
    "    for w, g, m, n in zip(words, gold, my_pred, nltk_pred):\n",
    "        my_ok = \"âœ“\" if g == m else \"âœ—\"\n",
    "        nltk_ok = \"âœ“\" if g == n else \"âœ—\"\n",
    "        # Only print mismatches (where at least one tagger is wrong)\n",
    "        if g != m or g != n:\n",
    "            print(f\"{w:<18} {g:>8} {m:>10} {n:>12} {my_ok:>7} {nltk_ok:>9}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
